{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eca6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35daf4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_words = {\n",
    "    'fuck': 'function',\n",
    "    'awful': 'array',\n",
    "    'bitch': 'bert',\n",
    "    'sexist': 'linkedlist',\n",
    "    'shit': 'git',\n",
    "    'slut': 'struct',\n",
    "    'sexism': 'lexism',\n",
    "    'whore': 'core',\n",
    "    'kitchen': 'knn',\n",
    "    'sex': 'sets',\n",
    "    'hoe': 'hacker',\n",
    "    'devil': 'dictionary',\n",
    "    'hate' : 'hash'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51dd6a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=30000, output_dim=20, input_length=20))\n",
    "model.add(Bidirectional(LSTM(units=100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4f5117",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('sexismdataset.csv')\n",
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d68fe9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\baz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b881de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(text):\n",
    "\n",
    "  cleanr = re.compile('<[^>]*>')           # remove html\n",
    "  cleantext = re.sub(cleanr, ' ', text)\n",
    "\n",
    "  cleantext = re.sub(\"[-]\", \" \" , cleantext)   # remove - sign\n",
    "\n",
    "  cleantext = re.sub(\"[^A-Za-z ]\", \" \" , cleantext)  # remove evey character except alphabet\n",
    "  cleantext = cleantext.lower()\n",
    "\n",
    "  words = nltk.tokenize.word_tokenize(cleantext)\n",
    "  words_new = [i for i in words if i not in stop_words]\n",
    "\n",
    "  w = [lemmatizer.lemmatize(word) for word in words_new if len(word)>2]\n",
    "\n",
    "  return ' '.join(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d28e1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Text']=dataset['Text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e8ffdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "465/465 [==============================] - 16s 24ms/step - loss: 0.0327 - accuracy: 0.9888\n",
      "Epoch 2/10\n",
      "465/465 [==============================] - 11s 24ms/step - loss: 0.0247 - accuracy: 0.9919\n",
      "Epoch 3/10\n",
      "465/465 [==============================] - 11s 24ms/step - loss: 0.0202 - accuracy: 0.9932\n",
      "Epoch 4/10\n",
      "465/465 [==============================] - 12s 26ms/step - loss: 0.0177 - accuracy: 0.9939\n",
      "Epoch 5/10\n",
      "465/465 [==============================] - 13s 28ms/step - loss: 0.0158 - accuracy: 0.9952\n",
      "Epoch 6/10\n",
      "465/465 [==============================] - 13s 28ms/step - loss: 0.0143 - accuracy: 0.9957\n",
      "Epoch 7/10\n",
      "465/465 [==============================] - 13s 27ms/step - loss: 0.0150 - accuracy: 0.9950\n",
      "Epoch 8/10\n",
      "465/465 [==============================] - 13s 28ms/step - loss: 0.0133 - accuracy: 0.9956\n",
      "Epoch 9/10\n",
      "465/465 [==============================] - 13s 27ms/step - loss: 0.0117 - accuracy: 0.9960\n",
      "Epoch 10/10\n",
      "465/465 [==============================] - 13s 27ms/step - loss: 0.0107 - accuracy: 0.9967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fad15019f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=30000)\n",
    "tokenizer.fit_on_texts(dataset['Text'])\n",
    "sequences = tokenizer.texts_to_sequences(dataset['Text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=20)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(padded_sequences, dataset['oh_label'], epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd4dbae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be a sexist or not, space is yours this bitch is a slut\n",
      "1/1 [==============================] - 0s 374ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'this bert is a struct'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_curse_words(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=20)\n",
    "    \n",
    "    prediction = model.predict(padded_sequence)[0][0]\n",
    "    if prediction < 0.5:\n",
    "        return text\n",
    "    \n",
    "    for curse_word, alternative_word in alternative_words.items():\n",
    "        text = text.replace(curse_word, alternative_word)\n",
    "    \n",
    "    return text\n",
    "\n",
    "sample=input('be a sexist or not, space is yours ')\n",
    "replace_curse_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c11a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
