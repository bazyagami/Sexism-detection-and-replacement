{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf6cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4daeb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_words = {\n",
    "    'fuck': 'function',\n",
    "    'awful': 'array',\n",
    "    'bitch': 'bert',\n",
    "    'sexist': 'linkedlist',\n",
    "    'shit': 'git',\n",
    "    'slut': 'struct',\n",
    "    'sexism': 'lexism',\n",
    "    'whore': 'core',\n",
    "    'kitchen': 'knn',\n",
    "    'sex': 'sets',\n",
    "    'hoe': 'hacker',\n",
    "    'devil': 'dictionary',\n",
    "    'hate' : 'hash'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce6f8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=30000, output_dim=20, input_length=20))\n",
    "model.add(LSTM(units=100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da5965f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('sexismdataset.csv')\n",
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16990f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sushm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c574935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(text):\n",
    "\n",
    "  cleanr = re.compile('<[^>]*>')           # remove html\n",
    "  cleantext = re.sub(cleanr, ' ', text)\n",
    "\n",
    "  cleantext = re.sub(\"[-]\", \" \" , cleantext)   # remove - sign\n",
    "\n",
    "  cleantext = re.sub(\"[^A-Za-z ]\", \" \" , cleantext)  # remove evey character except alphabet\n",
    "  cleantext = cleantext.lower()\n",
    "\n",
    "  words = nltk.tokenize.word_tokenize(cleantext)\n",
    "  words_new = [i for i in words if i not in stop_words]\n",
    "\n",
    "  w = [lemmatizer.lemmatize(word) for word in words_new if len(word)>2]\n",
    "\n",
    "  return ' '.join(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e20882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Text']=dataset['Text'].apply(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cc16f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "465/465 [==============================] - 31s 41ms/step - loss: 0.3833 - accuracy: 0.8411\n",
      "Epoch 2/10\n",
      "465/465 [==============================] - 19s 41ms/step - loss: 0.1836 - accuracy: 0.9287\n",
      "Epoch 3/10\n",
      "465/465 [==============================] - 19s 42ms/step - loss: 0.1157 - accuracy: 0.9562\n",
      "Epoch 4/10\n",
      "465/465 [==============================] - 16s 34ms/step - loss: 0.0764 - accuracy: 0.9732\n",
      "Epoch 5/10\n",
      "465/465 [==============================] - 15s 33ms/step - loss: 0.0584 - accuracy: 0.9791\n",
      "Epoch 6/10\n",
      "465/465 [==============================] - 15s 33ms/step - loss: 0.0462 - accuracy: 0.9839\n",
      "Epoch 7/10\n",
      "465/465 [==============================] - 15s 33ms/step - loss: 0.0371 - accuracy: 0.9869\n",
      "Epoch 8/10\n",
      "465/465 [==============================] - 15s 33ms/step - loss: 0.0327 - accuracy: 0.9878\n",
      "Epoch 9/10\n",
      "465/465 [==============================] - 15s 33ms/step - loss: 0.0316 - accuracy: 0.9893\n",
      "Epoch 10/10\n",
      "465/465 [==============================] - 15s 33ms/step - loss: 0.0264 - accuracy: 0.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bc4e07e8e0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=30000)\n",
    "tokenizer.fit_on_texts(dataset['Text'])\n",
    "sequences = tokenizer.texts_to_sequences(dataset['Text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=20)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(padded_sequences, dataset['oh_label'], epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b21c04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be a sexist or not, space is yours 69976 whats up bitch?\n",
      "1/1 [==============================] - 0s 487ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'69976 whats up bert?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_curse_words(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=20)\n",
    "    \n",
    "    prediction = model.predict(padded_sequence)[0][0]\n",
    "    if prediction < 0.5:\n",
    "        return text\n",
    "    \n",
    "    for curse_word, alternative_word in alternative_words.items():\n",
    "        text = text.replace(curse_word, alternative_word)\n",
    "    \n",
    "    return text\n",
    "\n",
    "sample=input('be a sexist or not, space is yours ')\n",
    "replace_curse_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d25d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
